## 1. Подключение к jump node

1. Подключаемся к jump node (tmpl-jn), используя SSH.  
2. Переключаемся на пользователя `hadoop`:

   ```bash
   su hadoop
   cd ~
   ```

---

## 2. Проверка и активация переменных окружения

Перед установкой Airflow необходимо убедиться, что необходимые переменные окружения настроены. Мы ранее добавили их в `~/.profile`, поэтому достаточно выполнить:

```bash
source .profile
```

Убедитесь, что переменные окружения установились корректно (особенно `SPARK_HOME`, `HADOOP_HOME`, `JAVA_HOME` и пути в `PATH`):

```bash
echo $HADOOP_HOME
echo $JAVA_HOME
echo $SPARK_HOME
```

### Необходимые переменные окружения

```bash
export HADOOP_HOME=/home/hadoop/hadoop-3.4.0
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

export HIVE_HOME=/home/hadoop/apache-hive-4.0.0-alpha-2-bin
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib/*
export PATH=$PATH:$HIVE_HOME/bin

export SPARK_HOME=/home/hadoop/spark-3.5.3-bin-hadoop3
export HADOOP_CONF_DIR="/home/hadoop/hadoop-3.4.0/etc/hadoop"
export SPARK_LOCAL_IP=192.168.1.142
export PYTHONPATH=$(ZIPS=("$SPARK_HOME"/python/lib/*.zip); IFS=:; echo "${ZIPS[*]}"):$PYTHONPATH
export PATH=$SPARK_HOME/bin:$PATH
```

---

## 3. Активация виртуального окружения Python

В ранее созданном виртуальном окружении будут устанавливаться все Python-зависимости Airflow. Активируем его:

```bash
source venv/bin/activate
```
---

## 4. Установка Airflow

Установим Airflow версии 2.10.3 (как указано в примере) с дополнительными зависимостями для Celery:

```bash
pip install "apache-airflow[celery]==2.10.3" \
  --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.3/constraints-3.12.txt"
```

*Почему именно с Celery?*  
Celery – это система распределённого планирования задач, которую Airflow может использовать в качестве бэкенда для выполнения DAGов (вместо локального исполнителя).  
*Почему constraints?*  
Airflow жёстко контролирует версии пакетов-зависимостей. Указание constraints гарантирует, что будут установлены совместимые версии библиотек.

---

## 5. Создание пользователя Airflow

Создадим учётную запись административного пользователя для Airflow:

```bash
airflow users create \
  --role Admin \
  --username newadmin \
  --email admin \
  --firstname admin \
  --lastname admin \
  --password admin
```

Этот пользователь позволит заходить в веб-интерфейс Airflow с правами администратора.

---

## 6. Запуск Airflow

Для быстрого запуска (в том числе для теста или в среде разработки) используется команда:

```bash
airflow standalone
```

- При первом запуске будет инициализирована база данных Airflow и запущен локальный веб-сервер (по умолчанию на порте `8080`).
---

## 7. Проброс порта

Чтобы получить доступ к Airflow UI из локального браузера, настроим проброс порта `8080`:

```bash
ssh -L 8080:127.0.0.1:8080 176.109.91.34
```

**Примечание:**  
- Здесь `176.109.91.34` – это публичный IP вашей jump node (tmpl-jn).  
- После этой команды, когда вы откроете в браузере `http://localhost:8080`, вы попадёте в веб-интерфейс Airflow, работающий на удалённом сервере.

---

## 8. Создание директории для DAG-файлов

Airflow по умолчанию ищет DAG-файлы в папке, указанной в `airflow.cfg` (параметр `dags_folder`). Обычно это `~/airflow/dags`. Убедимся, что она существует и создадим её при необходимости:

```bash
mkdir -p ~/airflow/dags
```

---

## 9. Настройка конфигурации Airflow

Файл конфигурации Airflow находится в `~/airflow/airflow.cfg`. Проверьте, что в нём указано:

```cfg
dags_folder = /home/hadoop/airflow/dags
```

Чаще всего этот путь устанавливается по умолчанию, но лучше убедиться, что вы используете именно `~/airflow/dags`.

---

## 10. Загрузка/копирование DAG-файла

Теперь, когда всё готово, скопируйте свой DAG-файл (в примере он называется `dag.py`) в папку DAGs:

```bash
cp dag.py ~/airflow/dags
```

После этого Airflow автоматически подхватит DAG (примерно в течение 30 секунд, если работает в режиме `standalone`).

---

## 11. Проверка работы в веб-интерфейсе

1. Откройте в своём браузере `http://localhost:8080` (при запущенном SSH-тоннеле).  
2. Введите ранее созданные учётные данные (`newadmin` / `admin`).  
3. Убедитесь, что в списке DAGов появился ваш скопированный DAG.  
4. Вы можете включить DAG (переключатель On/Off), смотреть логи задач, граф исполнения и т. д.
