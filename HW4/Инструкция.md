## 1. Подготовка Jump-ноде

На jump-ноде устанавливаем необходимые пакеты для работы с Python:

```bash
sudo apt install python3-pip
sudo apt install python3-venv
```

> **Пояснение:**  
> Устанавливаем `python3-pip` для управления Python-пакетами и `python3-venv` для создания изолированного виртуального окружения.

---

## 2. Переключение на пользователя Hadoop и установка Spark

Переключаемся на пользователя `hadoop` и загружаем архив Spark:

```bash
su hadoop

wget https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
tar -xzvf spark-3.5.3-bin-hadoop3.tgz
```

> **Пояснение:**  
> Загружается дистрибутив Apache Spark, собранный для Hadoop 3. После загрузки архив распаковывается в текущей директории.

---

## 3. Настройка Hive

### 3.1 Конфигурация hive-site.xml

В файле **hive-site.xml** необходимо добавить (или отредактировать) следующие свойства:

```xml
<property>
    <name>hive.metastore.uris</name>
    <value>thrift://tmpl-jn:9083</value>  <!-- Default metastore port -->
</property>

<property>
    <name>hive.metastore.thrift.bind.host</name>
    <value>0.0.0.0</value>  <!-- Allow remote connections -->
</property>
```

> **Пояснение:**  
> Свойство `hive.metastore.uris` указывает на URI Hive Metastore, а `hive.metastore.thrift.bind.host` позволяет принимать удалённые подключения (в данном случае – со всех IP).

### 3.2 Запуск Hive Metastore и HiveServer2

После внесения изменений в конфигурацию необходимо перезапустить Hive Server:

```bash
pkill -f hiveserver2
hive --service metastore -p 9083 &
hive --hiveconf hive.server2.enable.doAs=false --hiveconf hive.security.authorization.enable=false --service hiveserver2 1>> /tmp/hs2.log 2>> /tmp/hs2.log &
```

> **Пояснение:**  
> Здесь сначала завершается процесс `hiveserver2` (если он уже запущен), затем запускается metastore на порту 9083, и после — HiveServer2 с отключёнными настройками безопасности. Логи перенаправляются в файл `/tmp/hs2.log`.

---

## 4. Настройка переменных окружения

### 4.1 Настройка переменных для Hadoop и Hive

Экспортируем переменные, чтобы Spark мог корректно взаимодействовать с компонентами Hadoop и Hive:

```bash
export HADOOP_CONF_DIR="/home/hadoop/hadoop-3.4.0/etc/hadoop"
export HIVE_HOME="/home/hadoop/apache-hive-4.0.0-alpha-2-bin"
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib/*
export PATH=$PATH:$HIVE_HOME/bin
```

> **Пояснение:**  
> Переменные указывают на директории конфигурационных файлов Hadoop и Hive, а также добавляют бинарники Hive в системный PATH для удобного доступа.

### 4.2 Указание IP адреса для Spark

Сообщаем Spark, какой IP-адрес использовать:

```bash
export SPARK_LOCAL_IP=192.168.1.142
```

> **Пояснение:**  
> Эта переменная помогает Spark правильно определить сетевой интерфейс, что важно для распределённого взаимодействия между узлами.

### 4.3 Формирование SPARK_DIST_CLASSPATH

Перечисляем все необходимые библиотеки:

```bash
export SPARK_DIST_CLASSPATH="/home/hadoop/spark-3.5.3-bin-hadoop3/jars/*:/home/hadoop/hadoop-3.4.0/etc/hadoop:/home/hadoop/hadoop-3.4.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/common/*:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-3.4.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-3.4.0/share/hadoop/yarn/*:/home/hadoop/apache-hive-4.0.0-alpha-2-bin/*:/home/hadoop/apache-hive-4.0.0-alpha-2-bin/lib/*"
```

> **Пояснение:**  
> Эта переменная окружения объединяет все JAR-файлы из Spark, Hadoop и Hive, обеспечивая корректное разрешение зависимостей при запуске Spark-приложений.

### 4.4 Определение SPARK_HOME и настройка Python окружения

Переходим в директорию с распакованным Spark и устанавливаем SPARK_HOME:

```bash
cd spark-3.5.3-bin-hadoop3/
export SPARK_HOME=`pwd`
```

Настраиваем PYTHONPATH, чтобы использовать встроенные Python-библиотеки Spark:

```bash
export PYTHONPATH=$(ZIPS=("$SPARK_HOME"/python/lib/*.zip); IFS=:; echo "${ZIPS[*]}"):$PYTHONPATH
export PATH=$SPARK_HOME/bin:$PATH
```

> **Пояснение:**  
> Эти переменные обеспечивают доступ к исполняемым файлам Spark и его Python API. PYTHONPATH включает пути к zip-архивам с необходимыми модулями.

---

## 5. Создание виртуального окружения Python и установка зависимостей

Возвращаемся в домашнюю директорию, создаём виртуальное окружение и устанавливаем необходимые пакеты:

```bash
cd ~
python3 -m venv venv
source venv/bin/activate
pip install -U pip 
pip install ipython
pip install onetl[files]
```

---

## 6. Загрузка и загрузка данных в HDFS

### 6.1 Создание директории на HDFS

Создадим директорию для входных данных:

```bash
hdfs dfs -mkdir /input
```

### 6.2 Скачивание датасета и загрузка его в HDFS

Скачиваем CSV-файл с данными (в данном случае – датасет Titanic):

```bash
wget https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv
hdfs dfs -put titanic.csv /input/titanic.csv
```

---

## 7. Запуск Spark и выполнение кода

Запускаем IPython:

```bash
ipython
```

В интерактивной сессии IPython выполняем код из файла **code.py**. Например, после выполнения кода можно проверить результат следующим образом:

```python
df = spark.sql("SELECT * FROM test.spark_partitions LIMIT 5")
df.show()
```

Ожидаемый вывод:

```
+--------+--------------------+------+----+-----+-----+------+------+ 
|Survived|                Name|   Sex| Age|SibSp|Parch|  Fare|Pclass|
+--------+--------------------+------+----+-----+-----+------+------+ 
|       0|Mr. Owen Harris B...|  male|22.0|    1|    0|  7.25|     3|
|       1|Miss. Laina Heikk...|female|26.0|    0|    0|7.925|     3|
|       0|Mr. William Henry...|  male|35.0|    0|    0|  8.05|     3|
|       0|     Mr. James Moran|  male|27.0|    0|    0|8.4583|     3|
|       0|Master. Gosta Leo...|  male| 2.0|    3|    1|21.075|     3|
+--------+--------------------+------+----+-----+-----+------+------+ 
```

> **Пояснение:**  
> Здесь выполняется SQL-запрос к таблице `test.spark_partitions` для выборки первых 5 строк. Вывод подтверждает корректность интеграции Spark с Hive.

---

## 8. Проверка через Hive (Beeline)

Также можно проверить данные через Hive, используя Beeline:

```bash
beeline -u jdbc:hive2://tmpl-jn:10000
```

После подключения выполняем запрос:

```sql
SELECT * FROM test.spark_partitions LIMIT 5;
```

Ожидаемый вывод:

```
+-----------+--------------------------------+---------+-------+--------+--------+---------+---------+
| Survived  |              Name              |   Sex   |  Age  | SibSp  | Parch  |  Fare   | Pclass  |
+-----------+--------------------------------+---------+-------+--------+--------+---------+---------+
| 0         | Mr. Owen Harris Braund         | male    | 22.0  | 1      | 0      | 7.25    | 3       |
| 1         | Miss. Laina Heikkinen          | female  | 26.0  | 0      | 0      | 7.925   | 3       |
| 0         | Mr. William Henry Allen        | male    | 35.0  | 0      | 0      | 8.05    | 3       |
| 0         | Mr. James Moran                | male    | 27.0  | 0      | 0      | 8.4583  | 3       |
| 0         | Master. Gosta Leonard Palsson  | male    | 2.0   | 3      | 1      |21.075   | 3       |
+-----------+--------------------------------+---------+-------+--------+--------+---------+---------+
```